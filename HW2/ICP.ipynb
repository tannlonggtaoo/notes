{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get canonical pts\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from PIL import Image\n",
    "import pickle\n",
    "\n",
    "model_dir = '../../models/'\n",
    "dae_dir = 'visual_meshes/visual.dae'\n",
    "png_dir = 'visual_meshes/texture_map.png'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get id-name pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "csv_dir = '../../training_data/objects_v1.csv'\n",
    "obj_data = pd.read_csv(csv_dir,sep=',')\n",
    "id2name = list(obj_data['object']) # begins at 0\n",
    "id2symm = list(obj_data['geometric_symmetry'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load mesh and sample point cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import trimesh\n",
    "from trimesh import viewer\n",
    "from trimesh import sample\n",
    "\n",
    "def FPS(ptcloud,count):\n",
    "    U = ptcloud\n",
    "    S = []\n",
    "    dist_to_S = np.full(len(U),np.inf)\n",
    "    \n",
    "    newpt_idx = 0\n",
    "\n",
    "    for _ in range(count):\n",
    "        S.append(U[newpt_idx])\n",
    "        dist_offer = np.sum((U - U[newpt_idx])**2,axis=1)\n",
    "        dist_to_S = np.min(np.stack((dist_offer,dist_to_S)),axis=0)\n",
    "        # find new point\n",
    "        newpt_idx = np.argmax(dist_to_S)\n",
    "    \n",
    "    return np.array(S)\n",
    "\n",
    "def get_canonical_pcd(obj_id,count):\n",
    "    obj_name = id2name[obj_id]\n",
    "\n",
    "    dae_f = open(os.path.join(model_dir+obj_name,dae_dir),'rb')\n",
    "    png_resolver = trimesh.resolvers.FilePathResolver(os.path.join(model_dir+obj_name,png_dir))\n",
    "    scenemesh = trimesh.exchange.dae.load_collada(dae_f,png_resolver)\n",
    "    geo = list(scenemesh['geometry'].values())[0]\n",
    "    mesh = trimesh.Trimesh(geo['vertices'],geo['faces'],vertex_normals=geo['vertex_normals'])\n",
    "    sample_pcd,_ = sample.sample_surface(mesh, 10*count, face_weight=None, sample_color=False)\n",
    "    pt_cloud_fps = FPS(sample_pcd,count)\n",
    "    return pt_cloud_fps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    }
   ],
   "source": [
    "import open3d\n",
    "def showpcd(pcd):\n",
    "    points = open3d.utility.Vector3dVector(pcd)\n",
    "    _pcd = open3d.geometry.PointCloud()\n",
    "    _pcd.points = points\n",
    "    open3d.visualization.draw_geometries([_pcd])\n",
    "# showpcd(get_canonical_pcd(2,10000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "NUM_OBJECTS = 79\n",
    "canonical_pcds_filename = 'canonical_pcds.pkl'\n",
    "if os.path.exists(canonical_pcds_filename):\n",
    "    with open(canonical_pcds_filename, 'rb') as f:\n",
    "        canonical_pcds = pickle.load(f)\n",
    "else:\n",
    "    canonical_pcds = []\n",
    "    for id in tqdm(range(NUM_OBJECTS)):\n",
    "        canonical_pcds.append(get_canonical_pcd(id,5000))\n",
    "    with open(canonical_pcds_filename, 'wb') as f:\n",
    "        pickle.dump(canonical_pcds, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pickle(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "training_data_dir = \"../../training_data/v2.2\"\n",
    "split_dir = \"../../training_data/splits/v2\"\n",
    "\n",
    "def get_scene(scenename):\n",
    "    # scenename refers to the i-j-k prefix\n",
    "    prefix = os.path.join(training_data_dir, scenename)\n",
    "    rgb = [prefix + \"_color_kinect.png\"]\n",
    "    depth = [prefix + \"_depth_kinect.png\"]\n",
    "    label = [prefix + \"_label_kinect.png\"]\n",
    "    meta = [prefix + \"_meta.pkl\"]\n",
    "    return rgb, depth, label, meta\n",
    "\n",
    "def get_split_files(split_name):\n",
    "    with open(os.path.join(split_dir, f\"{split_name}.txt\"), 'r') as f:\n",
    "        prefix = [os.path.join(training_data_dir, line.strip()) for line in f if line.strip() and int(line.strip()[0]) <= 2]\n",
    "        rgb = [p + \"_color_kinect.png\" for p in prefix]\n",
    "        depth = [p + \"_depth_kinect.png\" for p in prefix]\n",
    "        label = [p + \"_label_kinect.png\" for p in prefix]\n",
    "        meta = [p + \"_meta.pkl\" for p in prefix]\n",
    "    return rgb, depth, label, meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lift_seg(depth,label,meta):\n",
    "    # lifting\n",
    "    intrinsic = meta['intrinsic']\n",
    "    z = depth\n",
    "    v, u = np.indices(z.shape)\n",
    "    uv1 = np.stack([u + 0.5, v + 0.5, np.ones_like(z)], axis=-1)\n",
    "    scene_pcd = uv1 @ np.linalg.inv(intrinsic).T * z[..., None]  # [H, W, 3]\n",
    "    # segmenting\n",
    "    pcds = {}\n",
    "    scales = {}\n",
    "    for id in meta['object_ids']:\n",
    "        raw_pcd = scene_pcd[np.where(label==id)]\n",
    "        scales[id] = meta['scales'][id]\n",
    "        pcds[id] = (raw_pcd) / meta['scales'][id]\n",
    "        \n",
    "    return pcds, scales\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "def draw_registration_result(source, target, transformation=None, show_normal=False):\n",
    "    source_temp = copy.deepcopy(source)\n",
    "    target_temp = copy.deepcopy(target)\n",
    "    source_temp.paint_uniform_color([1, 0.706, 0])\n",
    "    target_temp.paint_uniform_color([0, 0.651, 0.929])\n",
    "    if transformation is not None:\n",
    "        source_temp.transform(transformation)\n",
    "    open3d.visualization.draw_geometries([source_temp, target_temp],point_show_normal=show_normal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_open3d_PointCloud(pcd_nparray):\n",
    "    points = open3d.utility.Vector3dVector(pcd_nparray.reshape([-1, 3]))\n",
    "    pcd = open3d.geometry.PointCloud()\n",
    "    pcd.points = points\n",
    "    pcd.estimate_normals(search_param=open3d.geometry.KDTreeSearchParamHybrid(radius=0.1, max_nn=30))\n",
    "    return pcd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scene_to_pcds(scenename):\n",
    "    rgb_files, depth_files, label_files, meta_files = get_scene(scenename)\n",
    "    rgb = np.array(Image.open(rgb_files[0])) / 255\n",
    "    depth = np.array(Image.open(depth_files[0])) / 1000\n",
    "    label = np.array(Image.open(label_files[0]))\n",
    "    meta = load_pickle(meta_files[0])\n",
    "\n",
    "    poses = np.array([meta['extrinsic']@meta['poses_world'][idx] for idx in meta['object_ids']]) # ground truth\n",
    "\n",
    "    pcds, scales = lift_seg(depth,label,meta)\n",
    "\n",
    "    return meta['object_ids'], pcds, scales, poses\n",
    "\n",
    "#name = '1-1-1'\n",
    "#ids,pcds,scales,poses = scene_to_pcds(name)\n",
    "#_, _, _, meta_files = get_scene(name)\n",
    "#meta = load_pickle(meta_files[0])\n",
    "#k = 1\n",
    "#draw_registration_result(get_open3d_PointCloud(canonical_pcds[ids[k]]),get_open3d_PointCloud(pcds[ids[k]]))\n",
    "#draw_registration_result(get_open3d_PointCloud(canonical_pcds[ids[k]]),get_open3d_PointCloud(pcds[ids[k]]),meta['poses_world'][ids[k]])\n",
    "#draw_registration_result(get_open3d_PointCloud(canonical_pcds[ids[k]]),get_open3d_PointCloud(pcds[ids[k]]),poses[k])\n",
    "#poses[k][:3,3] = 2*poses[k][:3,3] \n",
    "#draw_registration_result(get_open3d_PointCloud(canonical_pcds[ids[k]]),get_open3d_PointCloud(pcds[ids[k]]),poses[k])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**An awful mistake in ground-truth data: some transation vector lost a factor of 2...**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ICP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### init estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_point_cloud(pcd, voxel_size):\n",
    "    #print(\":: Downsample with a voxel size %.3f.\" % voxel_size)\n",
    "    pcd_down = pcd.voxel_down_sample(voxel_size)\n",
    "\n",
    "    radius_feature = voxel_size * 5\n",
    "    #print(\":: Compute FPFH feature with search radius %.3f.\" % radius_feature)\n",
    "    pcd_fpfh = open3d.pipelines.registration.compute_fpfh_feature(\n",
    "        pcd_down,\n",
    "        open3d.geometry.KDTreeSearchParamHybrid(radius=radius_feature, max_nn=100))\n",
    "    return pcd_down, pcd_fpfh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_global_registration(source_down, target_down, source_fpfh,\n",
    "                                target_fpfh, voxel_size):\n",
    "    distance_threshold = voxel_size * 1.5\n",
    "    #print(\":: RANSAC registration on downsampled point clouds.\")\n",
    "    #print(\"   Since the downsampling voxel size is %.3f,\" % voxel_size)\n",
    "    #print(\"   we use a liberal distance threshold %.3f.\" % distance_threshold)\n",
    "    result = open3d.pipelines.registration.registration_ransac_based_on_feature_matching(\n",
    "        source_down, target_down, source_fpfh, target_fpfh, True,\n",
    "        distance_threshold,\n",
    "        open3d.pipelines.registration.TransformationEstimationPointToPoint(False),\n",
    "        3, [\n",
    "            open3d.pipelines.registration.CorrespondenceCheckerBasedOnEdgeLength(\n",
    "                0.9),\n",
    "            open3d.pipelines.registration.CorrespondenceCheckerBasedOnDistance(\n",
    "                distance_threshold)\n",
    "        ], open3d.pipelines.registration.RANSACConvergenceCriteria(10000000, 0.999999))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare(T,T_pred,scale,symm):\n",
    "    t,t_pred = T[:3,3], T_pred[:3,3]\n",
    "    # see above\n",
    "    t_error = min(np.linalg.norm((t-t_pred)*scale),np.linalg.norm((2*t-t_pred)*scale))\n",
    "    R,R_pred = T[:3,:3], T_pred[:3,:3]\n",
    "\n",
    "    cos_R_error = np.clip(0.5*(np.trace(R@R_pred.T)-1),-1,1)\n",
    "\n",
    "    if 'inf' in symm:\n",
    "        R_error = 0\n",
    "    elif symm=='no':\n",
    "        R_error = np.arccos(cos_R_error) *180 / np.pi\n",
    "    elif '4' in symm:\n",
    "        R_error = (np.arccos(cos_R_error) *180 / np.pi)%90\n",
    "        R_error = min(abs(R_error),abs(90-R_error))\n",
    "    elif '2' in symm:\n",
    "        R_error = (np.arccos(cos_R_error) *180 / np.pi)%180\n",
    "        R_error = min(abs(R_error),abs(180-R_error))\n",
    "    return t_error,R_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 29/2800 [02:51<1:53:29,  2.46s/it] C:\\Users\\不灭的~1\\AppData\\Local\\Temp/ipykernel_43544/3211697845.py:15: RuntimeWarning: invalid value encountered in arccos\n",
      "  R_error = (np.arccos(0.5*(np.trace(R@R_pred.T)-1)) *180 / np.pi)%180\n",
      "C:\\Users\\不灭的~1\\AppData\\Local\\Temp/ipykernel_43544/3211697845.py:15: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  R_error = (np.arccos(0.5*(np.trace(R@R_pred.T)-1)) *180 / np.pi)%180\n",
      "  1%|          | 34/2800 [03:03<1:47:25,  2.33s/it]C:\\Users\\不灭的~1\\AppData\\Local\\Temp/ipykernel_43544/3211697845.py:12: RuntimeWarning: invalid value encountered in arccos\n",
      "  R_error = (np.arccos(0.5*(np.trace(R@R_pred.T)-1)) *180 / np.pi)%90\n",
      "C:\\Users\\不灭的~1\\AppData\\Local\\Temp/ipykernel_43544/3211697845.py:12: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  R_error = (np.arccos(0.5*(np.trace(R@R_pred.T)-1)) *180 / np.pi)%90\n",
      " 20%|█▉        | 550/2800 [46:45<4:09:39,  6.66s/it] "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "The Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "voxel_size = 0.005\n",
    "correct_cnt = 0\n",
    "total_cnt = 0\n",
    "with open(os.path.join(split_dir, \"val.txt\"), 'r') as f:\n",
    "    scenenames = [line.strip() for line in f if line.strip()]\n",
    "for scenename in tqdm(scenenames):\n",
    "    ids, pcds, scales, poses = scene_to_pcds(scenename)\n",
    "    for i,id in enumerate(ids):\n",
    "        source = get_open3d_PointCloud(canonical_pcds[id])\n",
    "        target = get_open3d_PointCloud(pcds[id])\n",
    "        #draw_registration_result(source,target,show_normal=False)\n",
    "\n",
    "        source_down, source_fpfh = preprocess_point_cloud(source,voxel_size)\n",
    "        target_down, target_fpfh = preprocess_point_cloud(target,voxel_size)\n",
    "        fpfh_init = execute_global_registration(source_down, target_down, source_fpfh, target_fpfh, voxel_size)\n",
    "        #draw_registration_result(source_down, target_down, fpfh_init.transformation)\n",
    "        \n",
    "        reg_refine = open3d.pipelines.registration.registration_icp(\n",
    "            source, target, 2*voxel_size, fpfh_init.transformation, \n",
    "            open3d.pipelines.registration.TransformationEstimationPointToPlane(), \n",
    "            open3d.pipelines.registration.ICPConvergenceCriteria(max_iteration=10000))\n",
    "        \n",
    "        dt,dtheta=compare(poses[i],reg_refine.transformation,scales[id],id2symm[id])\n",
    "        if abs(dt)<0.01 and dtheta<5:\n",
    "            correct_cnt+=1\n",
    "        total_cnt+=1\n",
    "print(f'Accuracy = {correct_cnt/total_cnt}')\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 ('env_for_torch_open3d')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bba6f57773676c374d55b47d0340819d56ef7d136b7a36f302ab8dd2177b3286"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
