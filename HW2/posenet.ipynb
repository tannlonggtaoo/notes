{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import Adam\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "csv_dir = '../../training_data/objects_v1.csv'\n",
    "obj_data = pd.read_csv(csv_dir,sep=',')\n",
    "id2name = list(obj_data['object']) # begins at 0\n",
    "id2symm = list(obj_data['geometric_symmetry'])\n",
    "training_data_dir = \"../../training_data/v2.2\"\n",
    "split_dir = \"../../training_data/splits/v2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def get_split_files(split_name):\n",
    "    # split_name = 'val' or 'train'\n",
    "    with open(os.path.join(split_dir, f\"{split_name}.txt\"), 'r') as f:\n",
    "        prefix = [os.path.join(training_data_dir, line.strip()) for line in f if line.strip() and int(line.strip()[0]) <= 2]\n",
    "        rgb_files = [p + \"_color_kinect.png\" for p in prefix]\n",
    "        depth_files = [p + \"_depth_kinect.png\" for p in prefix]\n",
    "        label_files = [p + \"_label_kinect.png\" for p in prefix]\n",
    "        meta_files = [p + \"_meta.pkl\" for p in prefix]\n",
    "    return rgb_files, depth_files, label_files, meta_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from PIL import Image\n",
    "def load_pickle(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "def lift_seg_withrgb(depth,label,meta,valid_id,rgb):\n",
    "    # lifting\n",
    "    intrinsic = meta['intrinsic']\n",
    "    z = depth\n",
    "    v, u = np.indices(z.shape)\n",
    "    uv1 = np.stack([u + 0.5, v + 0.5, np.ones_like(z)], axis=-1)\n",
    "    scene_pcd = uv1 @ np.linalg.inv(intrinsic).T * z[..., None]  # [H, W, 3]\n",
    "    scene_rgbpcd = np.concatenate((rgb,scene_pcd),axis=2)\n",
    "    # segmenting\n",
    "    rgbpcds = []\n",
    "    for id in valid_id:\n",
    "        raw_pcd = scene_rgbpcd[np.where(label==id)]\n",
    "        raw_pcd[:,3:] /= meta['scales'][id]\n",
    "        rgbpcds.append(raw_pcd)\n",
    "    return rgbpcds\n",
    "\n",
    "def to_rgbpcds(rgb_file, depth_file, label_file, meta_file):\n",
    "    rgb = np.array(Image.open(rgb_file)) / 255\n",
    "    depth = np.array(Image.open(depth_file)) / 1000\n",
    "    label = np.array(Image.open(label_file))\n",
    "    meta = load_pickle(meta_file)\n",
    "\n",
    "    valid_id = np.unique(label)\n",
    "    valid_id = np.intersect1d(valid_id,meta['object_ids'])\n",
    "    poses = np.array([meta['extrinsic']@meta['poses_world'][idx] for idx in valid_id]) # ground truth, some transactions are wrong\n",
    "\n",
    "    rgbpcds = lift_seg_withrgb(depth,label,meta,valid_id,rgb)\n",
    "\n",
    "    return valid_id, rgbpcds, poses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation data have been preprocessed...\n",
      "train data have been preprocessed...\n"
     ]
    }
   ],
   "source": [
    "def pt_select(pcd,cnt):\n",
    "    if len(pcd)>cnt:\n",
    "        return pcd[np.random.choice(len(pcd),cnt,replace=False)]\n",
    "    elif len(pcd)<cnt:\n",
    "        return pcd[np.random.choice(len(pcd),cnt,replace=True)]\n",
    "    else: return pcd\n",
    "\n",
    "def preprocess(split_name,pt_cnt_perpcd):\n",
    "    rgb_files, depth_files, label_files, meta_files = get_split_files(split_name)\n",
    "    valid_id_arr,rgbpcds_arr,poses_arr=[],[],[]\n",
    "    for i in tqdm(range(len(rgb_files))):\n",
    "        valid_id, rgbpcds, poses = to_rgbpcds(rgb_files[i], depth_files[i], label_files[i], meta_files[i])\n",
    "        rgbpcds = np.array([pt_select(pcd,pt_cnt_perpcd) for pcd in rgbpcds])\n",
    "        valid_id_arr.append(valid_id)\n",
    "        rgbpcds_arr.append(rgbpcds)\n",
    "        poses_arr.append(poses)\n",
    "    return np.concatenate(valid_id_arr,axis=0),np.concatenate(rgbpcds_arr,axis=0),np.concatenate(poses_arr,axis=0)\n",
    "\n",
    "\n",
    "processed_val_dir = '../../HW2DATA/processed_val.npz'\n",
    "if not os.path.exists(processed_val_dir):\n",
    "    valid_id_arr,rgbpcds_arr,poses_arr = preprocess('val',1024)\n",
    "    np.savez(processed_val_dir,valid_id=valid_id_arr,rgbpcd=rgbpcds_arr,pose=poses_arr)\n",
    "else: print('validation data have been preprocessed...')\n",
    "\n",
    "processed_train_dir = '../../HW2DATA/processed_train.npz'\n",
    "if not os.path.exists(processed_train_dir):\n",
    "    valid_id_arr,rgbpcds_arr,poses_arr = preprocess('train',1024)\n",
    "    np.savez(processed_train_dir,valid_id=valid_id_arr,rgbpcd=rgbpcds_arr,pose=poses_arr)\n",
    "else: print('train data have been preprocessed...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"valid_id_arr\" in globals(): del valid_id_arr\n",
    "if \"rgbpcds_arr\" in globals(): del rgbpcds_arr\n",
    "if \"poses_arr\" in globals(): del poses_arr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoseDataset(Dataset):\n",
    "    def __init__(self,datadir,thumbnail=False) -> None:\n",
    "        super().__init__()\n",
    "        metadata = np.load(datadir)\n",
    "        \n",
    "        self.valid_id = metadata['valid_id']\n",
    "        self.rgbpcd = metadata['rgbpcd']\n",
    "        self.pose = metadata['pose']\n",
    "\n",
    "        print(f'metadata from {datadir} loaded...')\n",
    "\n",
    "        if thumbnail:\n",
    "            self.valid_id=self.valid_id[:32]\n",
    "            self.rgbpcd=self.rgbpcd[:32]\n",
    "            self.pose=self.pose[:32]\n",
    "        \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.metadata['valid_id'])\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        return torch.Tensor(self.valid_id[index]),torch.Tensor(self.rgbpcd[index]),torch.Tensor(self.pose[index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def toRot(a,b):\n",
    "    a = F.normalize(a, dim=-1)\n",
    "    b = b - a * (a * b).sum(-1, keepdims=True)\n",
    "    b = F.normalize(b, dim=-1)\n",
    "    c = torch.cross(a, b, -1)\n",
    "    return torch.stack([a, b, c], dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 4, 4])\n",
      "tensor([[-0.5085, -0.5555, -0.6579,  0.3108],\n",
      "        [ 0.8511, -0.4400, -0.2863,  0.8844],\n",
      "        [-0.1304, -0.7056,  0.6965,  0.2931],\n",
      "        [ 0.0000,  0.0000,  0.0000,  1.0000]], grad_fn=<SelectBackward>)\n"
     ]
    }
   ],
   "source": [
    "class PoseNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        #self.emb = nn.Sequential()\n",
    "        self.bn0 = nn.BatchNorm1d(6)\n",
    "        self.emb1 = nn.Conv1d(6,32,1)\n",
    "        self.bn1 = nn.BatchNorm1d(32)\n",
    "        self.emb2 = nn.Conv1d(32,64,1)\n",
    "        self.bn2 = nn.BatchNorm1d(64)\n",
    "        self.emb3 = nn.Conv1d(64,1024,1)\n",
    "        self.bn3 = nn.BatchNorm1d(1024)\n",
    "\n",
    "        self.fc_stack = nn.Sequential(\n",
    "            nn.Linear(1024,256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256,128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128,9),\n",
    "        )\n",
    "\n",
    "    def forward(self,x):\n",
    "        # x.shape is [batchsize,pt_count,6]\n",
    "        # torch version is too low, so moveaxis is not available\n",
    "        x = x.permute(0,2,1)\n",
    "        center = x[:,3:6].mean(dim=-1)\n",
    "        x = F.relu(self.bn1(self.emb1(self.bn0(x))))\n",
    "        x = F.relu(self.bn2(self.emb2(x)))\n",
    "        x = F.relu(self.bn3(self.emb3(x)))\n",
    "\n",
    "        x = torch.max(x,2)[0]\n",
    "\n",
    "        x = self.fc_stack(x)\n",
    "        R = toRot(x[...,:3],x[...,3:6])\n",
    "        M = F.pad(R,(0,1,0,1))\n",
    "        M[:,:3,3] = x[...,6:] + center\n",
    "        M[:,3,3] = 1\n",
    "        return M\n",
    "\n",
    "def check_dim():\n",
    "    model = PoseNet()\n",
    "    x = torch.rand(32,512,6) # a little different\n",
    "    M = model(x)\n",
    "    print(M.shape)\n",
    "    print(M[0])\n",
    "\n",
    "check_dim()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "batchsize = 128\n",
    "epochs = 20\n",
    "lr = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metadata from ../../HW2DATA/processed_train.npz loaded...\n",
      "Example shape:torch.Size([32, 1024, 6])\n"
     ]
    }
   ],
   "source": [
    "trainset = PoseDataset(processed_train_dir,True)\n",
    "trainloader = DataLoader(trainset,batchsize,num_workers=4)\n",
    "model = PoseNet()\n",
    "optimizer = Adam(model.parameters(),lr=lr)\n",
    "\n",
    "print(f'Example shape:{trainset[0:batchsize][1].shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.,  0.,  0.],\n",
       "        [ 0.,  0., -1.],\n",
       "        [ 0.,  1.,  0.]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def rotation(w,theta):\n",
    "    if theta == 0:\n",
    "        return torch.eye(3)\n",
    "    # theta:degree\n",
    "    w = torch.FloatTensor(w)\n",
    "    w = F.normalize(w,dim=-1)\n",
    "    skew = torch.tensor([[0,-w[2],w[1]],[w[2],0,-w[0]],[-w[1],w[0],0]])\n",
    "    theta = theta * np.pi / 180\n",
    "    return torch.eye(3)+skew*np.sin(theta)+torch.mm(skew,skew)*(1-np.cos(theta))\n",
    "\n",
    "w = torch.tensor([1.,0.,0.])\n",
    "theta = 90\n",
    "rotation(w,theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "rot = {\n",
    "    'x2' : [rotation([1,0,0],0),rotation([1,0,0],180)],\n",
    "    'x4' : [rotation([1,0,0],0),rotation([1,0,0],90),rotation([1,0,0],180),rotation([1,0,0],270)],\n",
    "    'y2' : [rotation([0,1,0],0),rotation([0,1,0],180)],\n",
    "    'y4' : [rotation([0,1,0],0),rotation([0,1,0],90),rotation([0,1,0],180),rotation([0,1,0],270)],\n",
    "    'z2' : [rotation([0,0,1],0),rotation([0,0,1],180)],\n",
    "    'z4' : [rotation([0,0,1],0),rotation([0,0,1],90),rotation([0,0,1],180),rotation([0,0,1],270)],\n",
    "    'zinf' : [rotation([0,0,1],da) for da in range(0,360,5)],\n",
    "    'xinf' : [rotation([1,0,0],da) for da in range(0,360,5)],\n",
    "    'yinf' : [rotation([0,1,0],da) for da in range(0,360,5)],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00, -1.0000e+00, -1.2246e-16],\n",
      "        [ 0.0000e+00,  1.2246e-16, -1.0000e+00]])\n",
      "tensor([[-1.0000e+00, -1.2246e-16,  0.0000e+00],\n",
      "        [ 1.2246e-16, -1.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  1.0000e+00]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(180.)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def angleloss(R,R_pred):\n",
    "    return torch.abs(torch.acos(torch.clamp(0.5*(torch.trace(torch.mm(R,R_pred.transpose(0,1)))-1),-1,1)) *180 / np.pi)\n",
    "\n",
    "print(rot['x2'][1])\n",
    "print(rot['z2'][1])\n",
    "angleloss(rot['x2'][1],rot['z2'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.3430,  0.9358, -0.0814,  0.0219],\n",
      "         [ 0.7455,  0.2184, -0.6297,  0.0557],\n",
      "         [-0.5715, -0.2767, -0.7725,  0.6936],\n",
      "         [ 0.0000,  0.0000,  0.0000,  1.0000]]])\n",
      "tensor([[[ 0.8422, -0.5346, -0.0704,  0.1569],\n",
      "         [-0.4623, -0.6487, -0.6046, -0.0642],\n",
      "         [ 0.2775,  0.5417, -0.7934,  0.6419],\n",
      "         [ 0.0000,  0.0000,  0.0000,  1.0000]]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0.8451])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from itertools import permutations,product\n",
    "from functools import reduce\n",
    "\n",
    "C = np.pi / 180\n",
    "\n",
    "def ShapeAgnosticLoss(M_pred,M,valid_id):\n",
    "    R_pred,R = M_pred[:,:3,:3],M[:,:3,:3]\n",
    "    t_pred,t = M_pred[:,:3,3],M[:,:3,3]\n",
    "    t_error = torch.norm(t-t_pred,dim=-1) # approximation\n",
    "\n",
    "    symms = [id2symm[int(id)] for id in valid_id]\n",
    "\n",
    "    loss = torch.zeros_like(valid_id)\n",
    "    for i,symm in enumerate(symms):\n",
    "        if symm == 'no':\n",
    "            loss[i] = t_error + C*angleloss(R[i],R_pred[i])\n",
    "            continue\n",
    "        \n",
    "        symm_keys = symm.split('|')\n",
    "        results = []\n",
    "        if len(symm_keys)==0:\n",
    "            results=[torch.eye(3)]\n",
    "        elif len(symm_keys)==1:\n",
    "            results=rot[symm_keys[0]]\n",
    "        else:\n",
    "            for p in permutations(symm_keys):\n",
    "                factors = [rot[i] for i in p]\n",
    "                for factor_list in product(*factors):\n",
    "                    results.append(reduce(torch.mm, factor_list))\n",
    "\n",
    "        loss[i] = t_error + C*min(angleloss(R[i],torch.mm(R_pred[i],R_symm)) for R_symm in results)\n",
    "    return loss\n",
    "\n",
    "M1,M2 = trainset[0:1][2], trainset[1:2][2]\n",
    "print(M1)\n",
    "print(M2)\n",
    "ShapeAgnosticLoss(M1,M2,trainset[0:1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model,trainloader,optimizer,epoch_limit):\n",
    "    loss_record = []\n",
    "    model = model.to('cuda')\n",
    "    for e in range(epoch_limit):\n",
    "        loss_sum = 0\n",
    "        for t, (valid_id,x,y) in enumerate(trainloader):\n",
    "            model.train()\n",
    "            x = x.to('cuda')\n",
    "            y = y.to('cuda')\n",
    "            y_pred = model(x)\n",
    "\n",
    "            loss = ShapeAgnosticLoss(y_pred,y,valid_id)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            loss_sum += loss\n",
    "            if t % 1 == 0:\n",
    "                print('Epoch %d, Iteration %d, loss = %.4f' % (e, t, loss))\n",
    "        loss_record.append(loss_sum*trainloader.batch_size/len(trainloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(model,trainloader,optimizer,2)\n",
    "torch.save(model.state_dict(), 'model.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 ('env_for_torch_open3d')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bba6f57773676c374d55b47d0340819d56ef7d136b7a36f302ab8dd2177b3286"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
